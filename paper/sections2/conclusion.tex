\section{Conclusion}\label{sec:conclusion}
In this paper the author presented
a Python decorator leveraging previously designed test(s) to directly create 
unit tests at minimal development cost.
The trade-off is that this approach increases execution time of the function 
tests and, like AI-generated tests, such tests would still require review by the 
developer. The latter is true of any automatically generated test and 
the author proposed and demonstrated a variety of approaches to 
mitigate the increased runtime for existing tests.

The author proved his initial hypothesis that 
that unit tests could be
created programmatically by monitoring and capturing 
metadata from existing tests.

Although much work remains, the author maintains an optimistic outlook
that the testing paradigm presented here could prove valuable in the 
world of automated test generation due to the variety of successful 
tests noted above.  By generating unit tests from functional tests, 
not only are the unit tests created essentially for free,
but the quality (i.e. coverage) of other tests generated via BDD or ATDD 
processes can be assessed. This could enable rapid comparison
of the relative test coverage of various testing paradigms.

The code is available in a single Python file to permit 
easy integration into any supported Python project.
The author wishes to give back to the computing community
and therefore offers this project free and open source
in the hopes that it will be found useful and lay the ground
work for future research and application.
The author hopes the concepts described herein will be 
applied in other languages with strong metaprogramming support
such as C\#, Go, Zig, etc. Further information on the code and  documentation, please
find it online at
%
\begin{center}
  \textbf{LINK REDACTED FOR DOUBLE BLIND REVIEW}
\end{center}


% vim: spelllang=en_GB
