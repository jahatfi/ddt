\section{Introduction}\label{sec:introduction}

\subsection{Software Testing Paradigms}\label{sec:intro-3}

\subsubsection{Test Driven Development}\label{sec:intro-}
Advocates of Test Driven Development (TDD) such a Harry Percival
 \cite{percival2014test} argue that software developers should
follow the TDD process in which they develop software by first writing tests,
then the bare minimum amount of code to make those tests pass,
before beginning the cycle anew with more tests.
%
On page 22 \cite{percival2014test} Percival advocates both functional and
 unit tests, making the following distinction:
“functional tests test the application from the outside, from the point of view
of the user. Unit tests test the application from the inside,
from the point of view of the programmer.”
%
Later, on page 470 \cite{percival2014test} Percival defines an “integration test” as
one that depends on and interacts with some external system.
%
“Integration test” is sometimes used in common vernacular in the same way that
Percival uses “functional test”.
%
This paper will do the same, using the terms interchangably.  The author will 
even use the term "ad hoc test" in reference to informal tests used by the developer.  
The key asssumption for this paper is that there exists at least one passing 
test above the unit level that calls at least one internal function.  This 
paper will describe how the work takes advantage (to a point) of the 
comprehensive nature of higher-level tests in which multiple functions are 
executed, potentially with various inputs.
%
Previous academic research \cite{causevic2011factors, ramzan2024test} observed
 that industry TDD adoption is lower than desired,
reporting increased longer development cycles, skill issues, and legacy code as 
a few root factors, among others.
%
\subsubsection{Non-TDD Testing Paradigms}\label{sec:intro-}
In contrast to the TDD methodology in which only the developers write the tests
\cite{axelrod2018unit}, 
others \cite{brown2013agility, shahabuddin2016integration, moe2019comparative} propose a 
more customer-focused testing approaches such as Behavior-Driven Development (BDD)
or Acceptance Test Driven Development (ATDD) focused on on the software's behavior 
from the user's perspective. In these paradigms, non-developers play a role 
in creating the tests, such as a business analyst (BDD) \cite{barus2019implementation}
or a cross-functional team of developers, business analysts, and testers \cite{pugh2010lean}.
These advocates argue that such feature-first approaches are optimal for accelerating development 
and reducing a product's time to market due to the time-consuming nature of manually
developing unit tests \cite{kahur2023java, shahabuddin2016integration}.
Though Brown \textit{et. al.} acknowledge that integration and unit
testing may go hand-in-hand, they 
ultimately argue in favor of the former before the latter \cite{brown2013agility}.  
Although implied by Brown \textit{et. al.}, Shahabuddin \textit{et. al.} 
\cite{shahabuddin2016integration} go so far as to explicitly argue that unit
testing can be performed \textit{after} initial product delivery to the customer.
%
This research corresponds with the author’s personal observations that immature 
organizations (or solo developers on small or personal projects)
often don’t adhere to the  TDD approach, and end up writing the tests 
after the development of the code or fail to write tests at all. In such cases,
the hobbyist may focus primarily on function or integration tests
and opt not to develop unit tests, perhaps because the pros of unit tests
do not outweigh the cons compared to a higher level test.
%
The main pros and cons for functional versus unit tests are summarized very
briefly Table 1.
\vskip .2cm
\noindent\begin{tabular}{|>{\centering\arraybackslash}m{0.6cm}|>{\centering\arraybackslash}m{3.4cm}|>{\centering\arraybackslash}m{3.4cm}|}
    \hline
    \multirow{2}{*}{} & \textbf{Functional Test(s)} & \textbf{Unit Test(s)} \\
    \hline
    \textbf{Pros} & \begin{itemize}[leftmargin=*]
        \item More relevant for the customer
        \item Efficiently tests modules together
        \item Less dependent on the internal unit implementations
    \end{itemize} & \begin{itemize}[leftmargin=*]
        \item Verifies individual components
        \item Efficient when testing single components
    \end{itemize} \\
    \hline
    \textbf{Cons} & \begin{itemize}[leftmargin=*]
        \item Waste of time when testing small unit changes
        \item May not accurately identify root cause of failure
    \end{itemize} & \begin{itemize}[leftmargin=*]
        \item Time consuming to develop
        \item Easily broken by refactoring code
    \end{itemize} \\
    \hline
\end{tabular}
\captionof{table}{Pros and Cons of Functional and Unit Tests}
Despite the time-consuming nature of developing unit tests manually, both
TDD and integration-test-first proponents recognize value in \textit{having}
unit tests. By definition, any reduction in the cost of unit test creation drives 
up their return on investment.

\subsubsection{Test Coverage}\label{sec:intro-cov}
The term “coverage” refers to the measure of the completeness of a test.  
Coverage can be measured in various ways, two prominent metrics are branch coverage 
(what percent of if/else branches executed) and line coverage 
(what percent of lines in the function executed during the test) 
\cite{wang2024software}. The \textit{coverage} Python module can
be used to gather this information directly; in fact, it is often paired with 
\textit{pytest} to generate coverage tests for existing tests. This paper will focus 
on the use of the line coverage metric, both in implementation and 
as the standard for selecting which execution records to ultimately 
keep for the final generated tests.

\subsection{Previous work on automated test creation}\label{sec:intro-3}

Given the cost of creating valuable tests,
the body of academic work on generating them automatically 
via metaprogramming is extensive. Classic algorithms like search 
and randomization feature prominently in earlier work 
\cite{Luk22Pynguin0170}, with generative AI significantly increasing in 
popularity most recently
\cite{bhatia2023unit,takerngsaksiri2024tdd,wang2024software, kahur2023java}.
%
These AI methods generate tests based on a variety of inputs, typically the code
itself and some other input such as human prompts 
\cite{lahiri2023interactivecodegenerationtestdriven},
natural language requirements \cite{wang2024software}, or entire projects
\cite{rao2023cat}.  

While the results of such AI-generated tests are promising,
their coverage is not perfect \cite{kahur2023java} and still require significant
developer review and correction \cite{sundqvist2024ai}.  

Lemieux \textit{et. al.} propose a hybrid method, \textit{CodaMosa}, that 
combines search and mutation (classical approaches) with generative AI
\cite{lemieux2023codamosa}.  Their work is impressive but still relies on 
heuristic techniques that the work here does not. This author is also unclear 
what, if any, specialized hardware was required to use \textit{CodaMosa} given
it's reliance on a large language model (LLM).

\subsection{Initial motivation}\label{sec:intro-4}

The author began exploring this concept of automated test case generation
working on a personal project, a static analyzer he'd written for C source code.  As the
author did this for fun and personal education, he had not written any unit
tests.  Instead, the author carefully developed a sample input that caused large
portions of the code to execute (e.g. created high code coverage).  This guided
development until the code eventually produced a “good” output, i.e. an integration
test.
  
Unit tests would have greatly eased the refactoring process and the author 
hypothesized that the metaprogramming abilities of Python might empower creation
of unit tests from that sole existing test. This paper describes the author's
effort to prove this hypothesis.

\subsection{Organization of this paper}\label{sec:intro-5}

Section 2 of this paper describes the overall approach to building a
Python decorator to generate unit tests from existing tests. The subsequent
section enumerates both technical and philosophical assumptions made by the
author, as well as alternative approaches when those assumptions may not hold.
Section 4 describes the three different approaches taken by the author to 
evaluate the generated tests, followed by a section dedicated to further 
discussion of this work compared and constrasted to more related work.  A short
conclusion follows, trailed by a brief Acknowledgements section.

% vim: spelling=en_US