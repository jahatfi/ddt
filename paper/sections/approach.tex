\section{Recording the Execution of a Python Function}\label{sec:approach}

\subsection{Defining the required components to create a unit test}\label{sec:intro-1}

% Overview of concepts
The author set out to record the execution of Python functions
in such a way as to enable exact reproduction of that 
function call in a standalone unit test.  The following components
must be recorded in some fashion in order to do so:
\begin{enumerate}
  \item The function itself
  \item The arguments to the function, including kwargs
  \item Any relevant global state (e.g. variables)
  \item Exceptions raised
  \item Test coverage
  \item Files/databases read from and/or written to
  \item Data sent/received via a socket
\end{enumerate}

The last two are left for future work, but this paper demonstrates how to capture 
the first five. The subsections that follow discuss exactly how to 
access or determine this info and cache for subsequent creation of unit tests
with this information.
%%
%% General information about The decorator
%%
\subsection{Accessing the function and its arguments}\label{sec:approach-internal-1}

Python enables trivial access to a function and 
its arguments via the concept of decorator functions.

Not to be confused with the decorator pattern, a Python decorator is a 
function that calls another, thereby permitting the developer to place new code 
before and/or after calling the original “decorated” function \textit{f}.  
The decorator function has full access to both the decorated 
(or "wrapped" function) \textit{f} as well as all the 
arguments passed to \textit{f}, both \textit{args} and \textit{kwargs}.  In other 
languages this kind of wrapping is often referred to as 
“function hooking” or "function call interception" 
 \cite{kang2018function}. Any number of decorators can be applied to a function 
in Python, creating a figurative Russian nesting doll of
functions calling functions, each with the ability to access the 
arguments of the function it wraps and modify the 
return value before return. A Python decorator is applied with the \lq@\rq 
symbol as shown on line 20 of Listing 2\footnote{The author optimized this and other code snippets for display in 
this paper; therefore they may differ slightly from their original 
source found in the associated repository}:

\lstinputlisting[%
  language=Python,%
  numbers=left,
  caption={decorator.py: A sample decorator that takes one argument.},%
  label={lst:decorator},%
]{examples/decorator.py}

Running the code in Listing~ref{lst:decorator} yields the output shown in 
Listing 3. Note the apparently erroneous math (line 1,7 of 
Listing~\ref{lst:decorator-output}), as the \textit{add\_ints} function is 
unaware that its first argument was modified by the \text{inner\_most\_decorator}.

\lstinputlisting[%
  language=bash,%
  numbers=left,
  caption={Output of decorator.py},%
  label={lst:decorator-output},%
]{examples/decorator\_result.txt}

As shown in Listing 2, not only can the Python developer access the function
via the variable \textit{f} (e.g. line 14), the developer also has full
access to the variables passed to \textit{f}, and can make 
arbitrary changes to the arguments in a transparent way (line 11), 
i.e. the calling function would never know the arguments 
were modified before being passed to the callee function.

In addition to access to the function and its arguments,
decorators provide the power to insert code immediately before and after 
the function (lines 8-13, 15, respectively), including leveraging arbitrary
arguments (e.g. \textit{my\_int}) passed to the decorator itself.

A decorator like the one on line 5 of Listing~\ref{lst:decorator} permits
the passing of separate arguments to the decorator itself.  
If not extranous argument is needed, that \textit{outermost} decorator
is unnecessary.

The author uses such a decorator (specifically named 
\break
\textit{unit\_test\_generator\_decorator} in the referenced repository) to take
 a “before” and "after" snapshot of the arguments
before and after the function is called.

%
\subsection{Accessing relevant global state}\label{sec:approach-internal-2}

% Next, discuss the need to access global state (read/write).
In addition to the arguments passed directly to the function, any relevant 
global state must also be captured. "Relevant" here refers only to those 
global values read from and/or written to by the function.
The author's code focuses on variables (e.g. the int \textit{c} in the example above), 
disregarding imported modules such as \textit{re, os, etc}, detecting such
modules in a separate parsing step.
% Note that the dis module provides this option via dumping bytecode for f
Access to the global values is non-trival compared to accessing the function 
and its arguments, but still possible.  The first step is to use the \textit{dis}
module to programmatically disassemble the decorated function.  This is only 
required on initial execution of the decoratee as subsequent executions, if any, 
benefit from cached results of the disassembly.
Programmatic disassembly of \textit{add\_ints} function during execution is shown
in Listing 4:

\lstinputlisting[%
  language=TeX,%
  numbers=left,
  caption={Result of Programmatically Disassembling \textit{add\_ints}},%
  label={lst:decorator},%
]{examples/actual\_disassemble\_add\_ints.txt}

Note the LOAD\_GLOBAL command on line 6 of Listing 4 to load the value of 
\textit{c}.  Such global names \textit{read from} are saved for later reference.  
Likewise, names and values \textit{written to} via 
the STORE\_GLOBAL commands are also cached for later use.
%
Of note, the disassembly shown above differs from stand-alone disassembly of 
the same function in the Python interpreter (compare the listing above to the
\textit{examples/disassemble\_decorator\_with\_decorators.txt} file in the 
accompanying repository.)
During actual execution the author's code disassembles only the
\textit{add\_ints} function, after the decorators
have already executed, leaving only the original  \textit{add\_ints}
function for disassembly.
In contrast, disassembling the function in the static, non-executing context of
the Python interpreter reveals the code of all applied decorators.  
%
Values read from the global state must be recorded 
in order to monkeypatch them in the resulting unit test.
%
The decorated function must also record modified global values in
order to assert that the state was correctly changed by the function.

\subsection{Detecting exceptions}\label{sec:approach-internal-3}
Detecting exceptions is easiest of the three information capture steps.
Any exception can be detected with the code shown in Listing 
~\ref{exceptions}.

\begin{lstlisting}[language=Python, caption={Catching and recording exceptions}, label={exceptions}]
  try:
    # call the decorated function, e.g.
    f(args, kwargs)
  exception Exception as e:
    # Save the exception type and exception message
  \end{lstlisting}

\subsection{Determing Test Coverage}\label{sec:approach-internal-4}
The Python \textit{coverage}
module provides support for capturing line (and branch)-based coverage
as shown in Listing~\ref{lst:Coverage} \cite{Batchelder}.

\lstinputlisting[%
  language=Python,%
  numbers=left,
  caption={Calling a function and capturing the test coverage with the \textit{coverage} module.},%
  label={lst:Coverage},%
]{examples/coverage_example.py}

The documented approach to using the \textit{coverage} module 
saves results to a persistent database on the filesystem.
However, due to the already high overhead caused by author's work, he opted not 
to use the database option by it separately, 
opting not to use the default database \cite{Batchelder}.

\subsection{Summary of the approach}\label{sec:approach-internal-5}

The author uses all the methods discussed above to take a "before" and “after” 
snapshot of the arguments and relevant global state of each execution. 

Note that the state of mutable \textit{arguments} must also be captured 
\textit{after} the function executes as called functions may change mutable 
arguments that persist upon return to the caller. The return value or exception 
type and exception message are also captured, in addition to line test coverage. 
For each execution of a given function, an instance of the 
\textit{CoverageInfo} class is created and the fields populated, see 
Listing~\ref{lst:CoverageInfo}

\lstinputlisting[%
  language=Python,%
  numbers=left,
  caption={The CoverageInfo class caches all metadata associated with a single execution.},%
  label={lst:CoverageInfo},%
]{examples/coverage_info.py}
In addition, one instance of the FunctionMetaData classes shown in 
Listing~\ref{lst:FunctionMetaData} is populated for each decorated function.

\lstinputlisting[%
  language=Python,%
  numbers=left,
  caption={The FunctionMetaData class caches all metadata associated with
   a single function, to include all associated CoverageInfo classes (line 25)},%
  label={lst:FunctionMetaData},%
]{examples/function\_metadata.py}

\section{Programmatically Tuning the Decorator}\label{sec:decorator tuning}

\subsection{Selectively bypassing the decorator}\label{sec:tuning-1}

As the reader may imagine, the decorator described above adds significant overhead 
to the execution time of the overall test, as the function call is intercepted,
arguments inspected and copied, etc.  To mitigate this overhead the author provides 
a variety of methods to "bypass" the decorator, i.e. calling the decorated 
function and simply returning the result without further action.

The first and perhaps most obvious approach is to set a coverage threshold such 
that the decorator is effectively disabled once a desired level of coverage 
(e.g. 80\%) is achieved. 

One could also define a specific number 
of executions to capture, after which the decorator would be disabled. 
The author supports either option by setting either the \textit{percent\_coverage} or 
\textit{sample\_count} option to the decorator to the desired value.  Setting 
\textit{percent\_coverage} to a value greater than 100 will capture all 
executions of the decorated function.  Listing~\ref{lst:ProgrammaticallyTuning}
shows this code.
\lstinputlisting[%
  language=Python,%
  numbers=left,
  caption={Programmatically "bypassing" the decorator by immediately returning 
  the function results when desired coverage met},%
  label={lst:ProgrammaticallyTuning},%
]{examples/programmatictuning.py}

The careful reader may wonder if the \textit{sample\_count} option may result in 
duplicate tests cases, e.g. five test cases are captured but three are duplicates, 
yielding only three unique test cases.  This would in fact be the case, however, the author 
addressed this by hashing the inputs (global values, args and kwargs) and 
caching the hash in a set name \textit{hashed\_inputs}.  If a function has
already been called with the exact same inputs, the decorator immediately returns
the result in the same fashion as shown above. Thus, all test cases 
ultimately generated are unique.  This represents the last mitigation
currently in the code.  In future work the author hopes to return the cached 
result/raise the same exception of this execution to save even more overhead.  

It is also important to note that the decorator need only be applied until the 
test cases are generated and the developer is satisfied that the created unit
tests will suffice.  At that point the decorator can be completely removed
from the functional or integration test\footnote{Equivalently, 
\textit{sample\_count} and \textit{percent\_coverage} can both be set to 0.}, 
thereby restoring the runtime of
the original test back to its original, faster runtime.  

\subsection{Selectively keeping specific test cases}\label{sec:tuning-1}
Ideally, the same function will be executed multiple times during an ad hoc or 
function test to maximize coverage.  That said, the tester may wish to preserve 
a minimal number of test cases to avoid too much redundancy.  
%
One such approach is to drop the current execution
record if the current execution didn’t add any new coverage.  
Conversely, one can drop previous records if the current coverage supersedes
previous coverage.
There are a few ways to minimize duplicate coverage as explored below, 
but the reader should bear in mind:

\begin{enumerate}
  \item that \textit{fully} orthogonal test cases, as measured by coverage, are not feasible 
  (all test cases will cover the initial lines until the first conditional statement) 
  \item test cases with redundant coverage can still hold value
  \item some code such as function calls and regular expressions may have one 
  line in source code, but multiple branches in the underlying library or machine code.
\end{enumerate}

Nevertheless, the value added by redundant tests often plateaus 
\cite{lemieux2023codamosa}, hence the following sections
describe how to reduce such redundancy.  The first section describes
the technique employed by the author to balance memory usage and performance but
with intermediate complexity. The paragraph that follows describes a much simpler and faster
approach but offers poorer performance in that is rarely selects optimal test 
cases.  For the reader desiring the truly ideal solution, i.e. keep the fewest test cases that achieve
maximum coverage, a note on the proper algorithm to pursue such a goal completes
this section. This method was not selected to due its higher menory
requirements and higher complexity of implementation.
 
\subsubsection{Selected Approach: Individual Subsets}\label{sec:tuning-2}
One can maintain coverage information for previous
executions individually and check the current set of covered lines
against all previous coverage sets.  If the current coverage is
a subset of all existing records, it is discarded.  
Else, all previous records whose coverage is 
a proper subset of the current record are deleted (or \textit{pruned})
and the current record kept. This is more effort than the next approach but
retains fewer records with the same total coverage.  
A simple sample of the individual subset approach is shown in Table 2.

\vskip .2cm
\begin{tabular}{|>{\centering\arraybackslash}m{1.6cm}|>{\centering\arraybackslash}m{1.25cm}|>{\centering\arraybackslash}m{4.5cm}|}
    \hline
    \multirow{2}{*}{\textbf{Execution \#}} & \textbf{Coverage (line \#s)} & \textbf{Action} \\
    \hline
    1 & 1-5 & Keep this record, since it is the first one.\\
    \hline
    2 & 1-7 & Keep this and drop record \#1, as that coverage is only a subset of this coverage.\\
    \hline
    3 & 1-2 & Don’t keep this record; it is a subset of an existing record (\#2)\\
    \hline
\end{tabular}
\captionof{table}{Hypothetical scenario \#1 demonstrating record pruning by checking for subsets}
\vskip .2cm
Unfortunately, this subset approach still leaves room for redundant tests.  
Consider a new scenario:
\vskip .2cm
\begin{tabular}{|>{\centering\arraybackslash}m{1.6cm}|>{\centering\arraybackslash}m{1.25cm}|>{\centering\arraybackslash}m{4.5cm}|}
    \hline
    \multirow{2}{*}{\textbf{Execution \#}} & \textbf{Coverage (line \#s)} & \textbf{Action} \\
    \hline
    1 & 1-5 & Keep this record, since it is the first one.\\
    \hline
    2 & 6-10 & Keep this one, it covers new code.\\
    \hline
    3 & 5-6 & Keep this record, it’s not a subset of any previous record.\\
    \hline
\end{tabular}
\captionof{table}{Hypothetical scenario \#2 demonstrating record pruning by checking for subsets}
\vskip .2cm

Since the subset approach does not aggregate all the coverage into a 
single unified set as in the next approach, the record for execution \#3 would
be kept, but this would be redundant, as executions 1 and 2 already executed
lines 5 and 6, respectively.
The author implemented this approach such that setting the \textit{keep\_subsets}
options to True would not drop redundant records in case the user desired to keep
test cases that produce redundant coverage.  

\subsubsection{Check for new coverage via unified set}\label{sec:tuning-2}
Another approach is to maintain a unified set $U_t$ that is the union of all lines 
covered by all recorded tests from $S_0$ to $S_n$: 

\begin{equation*}
  U_t = S_0 \cup S_1 ...\cup... S_n
\end{equation*}
then only keeping the current record $S_i$ if any currently 
covered lines are not in the unified set:
\begin{equation*}
  S_i \not \subset U_t
\end{equation*}
then updating the unified set with the new coverage information like so:
\begin{equation*}
  U_t \Leftarrow U_t + S_i
\end{equation*} 
With this approach the individual coverage information $S_i$ is not maintained 
($U_t$ is updated, then $S_i$ is discarded), making it impossible to drop superseded 
coverage records over time.  100\% coverage would mean that:
\begin{equation*}
  U_f = U_t
\end{equation*} 
This approach is the simplest of the three described here but suffers from a 
lack of granularity: it's impossible to remove old records if new records have 
better coverage, as specific coverage per test is not recorded.

\subsubsection{Optimal Result by Solving the Minimum Set Cover Problem}\label{sec:tuning-2}
If the developer desires the best solution, e.g. to keep the bare minimum
number of test cases, all records must be cached initially.  
From there, the tester must solve the Minimum Set Cover Problem
\cite{hassin2005better} to select the minimum test cases.
This classic NP-Hard computer science problem involves finding the smallest number
of sets whose union is equal to or greater than (“covers”) some universe $U$ of elements.  
In this context, the coverage (set of line number executed) of each test is one set $S_i$,
and the set of all line numbers of a function is the universe $U_f$ to cover.  
As noted, this requires caching the unique coverage of each test, which may 
be prohibitive from a memory standpoint for large applications.
If memory is not a concern, this method may reduce the runtime 
compared to the alternate approaches described above. \footnote{Validating this hypothesis
could be an area for future work.} As the pruning 
only takes place after the tests are complete, the tests are not slowed as much
as the previous methods that prune during the tests.  That said, the sole
pruning operation will take comparatively longer as all the records must be 
analyzed all at once. The reader should be aware that due to the NP-Hard
nature of this problem, typical solutions are heuristic-based.
   
The author did not implement 
the Minimum Set Cover problem or its weighted variants, but encourages 
the interested reader to do so.  The author records the execution time of each 
execution as the \lq cost \rq. This value could be applied in a weighted 
variant of the Minimum Set Cover problem, i.e. select the "best" test cases
such that coverage is maximized and run time is minimized.

\section{Generating the Unit Tests via Metaprogramming}\label{sec:generating-tests}

After capturing the function metadata as documented in Listings 7 and 8, 
the next step was to programmatically generate the unit tests with all their 
captured test cases and place them in a file named 
\textit{test\_\$function\_name.py}. 
via a call to \textit{generate\_all\_tests\_and\_metadata} at the end of 
the functional test.
Calling this function eventually executes \textit{auto\_generate\_tests} 
that uses metaprogramming to:
\begin{enumerate}
  \item Build a list of imports
  \item Define global variables constant across all test cases
  \item Convert values to valid (i.e. canonical) Python code
  \item Build a comment specifying the level of coverage achieved\footnote{e.g. line 17 in Listing 13}
  \item Collect parameters to create a parameterized pytest test
  \item Monkeypatch all non-constant relevant globals read from
  \item Assert result is correct
  \item Assert modified globals are correct
  \item Assert expected exceptions and their messages are correct
  \item Write all of the above to a syntactically correct Python file
  \item Use subprocess to lint and format the result with \textit{black} and \textit{ruff}
\end{enumerate}

To aid the developer's troubleshooting efforts, the contents of each FunctionMetaData
class are also dumped to their own .json file, one per decorated function (hence
the \textit{and\_metadata} suffix noted above).  This file permits the 
developer to easily inspect the inputs, outputs, and other data
associated with the recording without the distraction of the test code.

Thus, if the function \textit{foo} is decorated with \textit{unit\_test\_decorator},
the generated unit test code can be found in \textit{test\_foo.py}, and the 
metadata can be found in a JSON file name named \textit{foo.json}

In addition to all of the above, significant development efforts went into 
serializing and deserializing Python code to and from strings.  Though
certainly not a panecea, the work here advances that done by 
Lemueux \textit{et. al.} \cite{lemieux2023codamosa} not only in terms of 
automated testing but also in terms of serialization capabilities.
% Discuss overhead and coverage

% Demonstrate how the tests themselves are created with metaprogramming

% vim: spelllang=en\_US
