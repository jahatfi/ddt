\section{Recording the Execution of a Python Function}\label{sec:approach}
% Overview of concepts
The author set out to record the execution of Python functions
in such a way as to enable exact reproduction of that 
function call at at later time.  The following components
must be recorded in some fashion in order to do so:
\begin{enumerate}
  \item The function itself
  \item The arguments to the function, including kwargs
  \item Any relevant global state \(e.g. variables\)
  \item Exceptions raised
  \item Test coverage
  \item Files/databases read from and/or written to
  \item Data sent/received via a socket
\end{enumerate}

The last two are left for future work, but this paper demonstrates how to use 
the initial components. The subsections that follow discuss exactly how to 
access or determine this info and cache for subsequent creation of unit tests
with this information.
%%
%% General information about The decorator
%%
\subsection{Accessing the function and its arguments}\label{sec:approach-internal-1}

Python enables trivial access to a function and 
its arguments by another function via the concept of 
decorator functions.

Not to be confused with the decorator pattern, a Python decorator is simply a 
function that calls another, thereby permitting the developer to place new code 
before and/or after calling the original “decorated” function.  
The decorator function has full access to both the decorated 
(or "wrapped" function), \textit{f} as well as all the 
arguments passed to \textit{f}, both args and kwargs.  In other 
languages this kind of wrapping is often referred to as 
“function hooking” or "function call interception" 
 \cite{kang2018function}. Any number of decorators can be applied to a function 
in Python, creating a figurative Russian nesting doll of
functions calling functions,th the ability to access the 
arguments and functions of the function below it, and modify the 
return value before return.

% Show the decorator syntax and what it actually looks like under the hood,
% noting that f, args, and kwargs are accessible
A Python decorator is applied with the '@' symbol as shown below
\footnote{The author optimized this and other code snippets for display in 
this paper; therefore they may differ slightly from their original 
source found in the associated repository}:

\lstinputlisting[%
  language=Python,%
  numbers=left,
  caption={decorator.py: A sample decorator that takes one argument.},%
  label={lst:decorator},%
]{examples/decorator.py}

Running the code above yields:

\lstinputlisting[%
  language=bash,%
  caption={Output of decorator.py},%
  label={lst:decorator},%
]{examples/decorator\_result.txt}

As demonstrated above, not only can the Python developer access the function
via the variable \textit{f}, the developer also has full
access to the variables passed to \textit{f}, and can make 
arbitrary changes to the arguments in a transparent way, 
i.e. the calling function would never know the arguments 
were modified before being passed to the callee function.

In addition to access to the function and its arguments,
developers can use decorators to insert code immediately before and after 
the function, including leveraging arbitrary arguments (e.g.
\textit{my\_int}) passed to the decorator itself.

The author uses such a decorator (specifically named 
\break
\textit{unit\_test\_generator\_decorator} in the referenced repository) to take
 a “before” and "after" snapshot of the arguments
before and after the function is called.

%
\subsection{Accessing relevant global state}\label{sec:approach-internal-2}

% Next, discuss the need to access global state (read/write).
In addition to the arguments passed directly to the function, any relevant 
global state must also be captured. "Relevant" here refers only to those 
global values read from and/or written to by the function.
The author's code focuses on variables (e.g. the int \textit{c} in the example above), 
disregarding imported modules such as \textit{re, os, etc}, detecting such
modules in a seperate parsing step.
% Note that the dis module provides this option via dumping bytecode for f
Access to the global values is non-trival compared to accessing the function 
and its arguments, but still possible.  The first step is to use the \textit{dis}
module to programmatically disassemble the decorating function.  This is only 
required on initial execution of the decoratee as subsequent executions, if any, 
benefit from cached results of the disassembly.
Programmatic disassembly of \textit{add\_ints} function during execution is shown below:

\lstinputlisting[%
  language=TeX,%
  caption={Result of Programmatically Disassembling \textit{add\_ints.py}},%
  label={lst:decorator},%
]{examples/actual\_disassemble\_add\_ints.txt}

Note the LOAD\_GLOBAL command to load the value of \textit{c}.  Any such global names 
are sanity checked against the \textit{\_\_globals\_\_} attribute of the 
disassembled function.
%
If the name is found in the \textit{\_\_globals\_\_} dictionary, its name and 
value is saved for later reference.  Likewise, names and values written to via 
the STORE\_GLOBAL commands are also parsed, verified to exist in the function's
 \textit{\_\_globals\_\_} attribute, and cached for later use if so.
%
Of note, the disassembly shown above differs from stand-alone disassembly of 
the same function in the Python interpreter (compare the listing above to the
 examples/disassemble\_decorator\_with\_decorators.txt file in the 
 accompanying repository.)
This reason for this is that during actual execution the author's code
 disassembles only the \textit{add\_ints} function, after the decorators
  have already been unwrapped.
In contrast, disassembling the function in the static, non-executing context of
the Python interpreter reveals the code of the applied decorators.  
%
For the global state read from, those values must be recorded 
in order to monkeypatch them during the unit test programmatically 
created by this work.
%
For relevant global state written to, the decorated function must also record
the new state in order to assert that the state was correctly changed by the function.

\subsection{Detecting exceptions}\label{sec:approach-internal-3}
Detecting exceptions is perhaps the easiest of the three information capture steps.
Any exception can of course be detected with the simple anti-pattern:

\begin{lstlisting}[language=Python, caption={Catching and recording exceptions}]
  try:
    # call the decorated function, e.g.
    f(args, kwargs)
  exception Exception as e:
    # Save the exception type and exception message
  \end{lstlisting}

\subsection{Determing Test Coverage}\label{sec:approach-internal-4}
As the purpose of this work is to advance the science of automated unit test 
creation, a key component is recording test coverage.  The Python \textit{coverage}
module provides support for just this task like so:

\lstinputlisting[%
  language=Python,%
  numbers=left,
  caption={Coverage: Calling a function and capturing the test coverage.},%
  label={lst:Coverage},%
]{examples/coverage.py}

However, given the non-standard
approach of this work compared to typical testing, the author programmatically
copied the results from the output of the coverage tool and cached it seperately, 
opting not to use the default coverage database persistantly.

\subsection{Summary of the approach}\label{sec:approach-internal-5}

The author uses all the methods discussed above to take a "before" and “after” 
snapshot of the arguments and relevant global state of each execution. 

(Note that the state of the arguments must also be captured after the function 
executes as called functions may change mutable arguments that persist upon 
return to the caller.) The return value or exception type and exception message
 are also captured, in addition to line test coverage. 
 For each execution of a given function, an instance of the 
 following class is created and the fields populated:

\lstinputlisting[%
  language=Python,%
  numbers=left,
  caption={CoverageInfo: The class that caches all metadata associated with a single execution.},%
  label={lst:CoverageInfo},%
]{examples/coverage_info.py}

In addition, one of each of the FunctionMetaData classes below is populated for each decorated function:

\lstinputlisting[%
  language=Python,%
  numbers=left,
  caption={FunctionMetaData: The class that caches all metadata associated with
   a single function, to include all associated CoverageInfo classes},%
  label={lst:FunctionMetaData},%
]{examples/function\_metadata.py}

\section{Programmatically Tuning the Decorator}\label{sec:decorator tuning}

\subsection{Selectively bypassing the decorator}\label{sec:tuning-1}

As the reader may imagine, the decorator described above adds significant overhead 
the execution time of the overall test, as the function call is intercepted,
arguments inspected, copied etc.  To mitigate this overhead the author provides 
a variety of methods to 
reduce this overhead by "bypassing" the decorator, i.e. calling the decorated 
function and simply returning the result without further action.

The first and perhaps most obvious approach is to set a coverage threshold such 
that the decorator is effectively disabled once a desired level of coverage 
(e.g. 80\%) is achieved. 

As a corollary, one could also define a specific number 
of executions to capture, after which the decorator would be disabled. 
The author supports either option by setting either the \textit{percent\_coverage} or 
\textit{sample\_count} option to the decorator to the desired value.  Setting 
\textit{percent\_coverage} to a value greater than 100 will capture all 
executions of the decorated function.
\lstinputlisting[%
  language=Python,%
  numbers=left,
  caption={Programmatically "bypassing" the decorator by immediately returning 
  the function results when desired coverage met},%
  label={lst:ProgrammaticallyTuning},%
]{examples/programmatictuning.py}

The careful reader may wonder if the \textit{sample\_count} option may result in 
duplicate tests cases.  This would in fact be the case, however, the author 
addressed this by hashing the inputs (global values, args and kwargs) and 
caching the hash in a set name \textit{hashed\_inputs}.  If a function has
already been called with the exact same inputs, the decorator immediately returns
the result in the same fashion as shown above, as the metadata associated with
 executing the decorated function
with those inputs was already captured and recorded.  Thus, all test cases 
generated are unique.  This represents the last mitigation
currently in the code.  In future work the author hopes to return the cached 
result/raise the same exception of this execution to save even more overhead.  

\subsection{Selectively keeping specific test cases}\label{sec:tuning-1}
As the same function may be executed multiple times during an ad hoc or 
function test, the user may opt not to record every execution by determining
 if the current execution didn’t add any new coverage. There are a few 
 ways to do this; a few are explored below.
 
 \subsubsection{Solving the Minimum Set Cover Problem}\label{sec:tuning-2}
 \hfill\\
If the developer caches all records initially then the Minimum Set Cover Problem
\cite{hassin2005better} is the exact algorithm required to select the minimum test cases.
This classic NP-Hard computer science problem involves finding the smallest number
  of sets whose union is equal to or greater than (“covers”) some universe $U$ of elements.  
  In this context, the coverage (set of line number executed) of each test is one set $S_i$,
 and the set of all line numbers of a function is the universe $U$ to cover. 

\subsubsection{Check for new coverage via unified set}\label{sec:tuning-2}
\hfill\\
One approach is to maintain a unified set $U$ that is the union of all lines 
covered by all recorded executions $S_0-n$: 

\begin{equation*}
  U = S_0 \cup S_1 ...\cup... S_n
\end{equation*}
then only keeping the current record $S_i$ if any currently 
covered lines are not in the unified set:
\begin{equation*}
  S_i \not \subset U
\end{equation*}
then updating the unified set with the new coverage information like so:
\begin{equation*}
  U += S_i
\end{equation*} 
With this approach the individual coverage information $S_i$ is not maintained 
($U$ is updated, then $S_i$ is discarded), making it impossible to drop superseded 
coverage records over time.  

\subsubsection{Individual Subset Approach}\label{sec:tuning-2}
\hfill\\
Alternatively, one can maintain coverage information for all previous
 executions individually and check the current set of covered lines
  against all previous coverage sets.  This would permit deletion of 
  previous records that covered a subset of the current execution.  

This is more effort than the previous approach but retains fewer records 
with the same total coverage.  A simple sample of the latter approach is shown here:

\vskip .2cm
\begin{tabular}{|>{\centering\arraybackslash}m{1.6cm}|>{\centering\arraybackslash}m{1.25cm}|>{\centering\arraybackslash}m{4.5cm}|}
    \hline
    \multirow{2}{*}{\textbf{Execution \#}} & \textbf{Coverage (line \#s)} & \textbf{Action} \\
    \hline
    1 & 1-5 & Keep this record, since it is the first one.\\
    \hline
    2 & 1-7 & Keep this and drop record \#1, as that coverage is only a subset of this coverage.\\
    \hline
    3 & 1-2 & Don’t keep this record; it is a subset of an existing record (\#2)\\
    \hline
\end{tabular}
\captionof{table}{Hypothetical scenario \#1 demonstrating record pruning by checking for subsets}
\vskip .2cm
Unfortunately, this subset approach still leaves room for redundant tests.  
Consider a new scenario:
\vskip .2cm
\begin{tabular}{|>{\centering\arraybackslash}m{1.6cm}|>{\centering\arraybackslash}m{1.25cm}|>{\centering\arraybackslash}m{4.5cm}|}
    \hline
    \multirow{2}{*}{\textbf{Execution \#}} & \textbf{Coverage (line \#s)} & \textbf{Action} \\
    \hline
    1 & 1-5 & Keep this record, since it is the first one.\\
    \hline
    2 & 6-10 & Keep this one, it covers new code.\\
    \hline
    3 & 5-6 & Keep this record, it’s not a subset of any previous record.\\
    \hline
\end{tabular}
\captionof{table}{Hypothetical scenario \#2 demonstrating record pruning by checking for subsets}
\vskip .2cm

Since the subset approach does not aggregate all  the coverage into a 
single unified set, the record for execution \#3 would be kept, but this would 
be redundant, as executions 1 and 2 already executed lines 5 and 6, respectively.

\subsubsection{Selected Approach}\label{sec:tuning-2}
\hfill\\

The author implemented the last approach such that setting the \textit{keep\_subsets}
options to True would not drop redundant records.  The author did not implement 
the Minimum Set Cover problem or its weighted variants, but encourages 
the interested reader to do so.  The author records the executing time of each 
execution as the 'cost'. This value could be applied in a weighted 
variant of the Minimum Set Cover problem, i.e. select the "best" test cases
such that coverage is maximized and run time is minimized.
   
\section{Generating the Unit Tests via Metaprogramming}\label{sec:generating-tests}

With all the metadata regarding the functions and their executions recorded, 
the next step was to programmatically generate the unit tests with all their 
captured test cases and place them in a file named 
\textit{test_FUNCTION\_NAME.py}. 
A simple call to \textit{generate\_all\_tests\_and\_metadata} at the end of 
the functional test will do just that.
 Calling this function eventually executes 
\textit{auto\_generate\_tests} that uses metaprogramming to:
\begin{enumerate}[leftmargin=*]
  \item Build a list of imports
  \item Define global variables constant across all test cases
  \item Convert values to valid (i.e. canonical) Python code
  \item Build a comment indicate the level of coverage achieved
  \item Collect parameters to create a parameterized pytest test
  \item Monkeypatch all non-constant relevant globals read from
  \item Assert result is correct
  \item Assert modified globals are correct
  \item Assert expected exceptions and their messages are correct
  \item Write all of the above to a syntactically correct Python file
  \item Use subprocess to lint and format the result with \textit{black} and \textit{ruff}
\end{enumerate}

To aid the developer's troubleshooting efforts, the contents of each FunctionMetaData
class are also dumped to their own .json file, one per decorated function (hence
the \textit{and\_metadata} suffix noted above).  This file permits the 
developer to easily inspect the inputs, outputs, and other data
associated with the recording without the distraction of the test code.

Thus, if the function \textit{foo} is decorated with \textit{unit\_test\_decorator},
the generated unit test code can be found in \textit{test\_foo.py}, and the 
metadata can be found in a JSON file name named \textit{foo.json}
% Discuss overhead and coverage

% Demonstrate how the tests themselves are created with metaprogramming

% vim: spelllang=en\_US
