\section{Assumptions}\label{sec:introduction}

The author began exploring this concept of automated test case generation
 working on a personal project, a toy static analyzer for C source code.  As the
 author did this for fun and personal education, he had not written any unit
 tests.  Instead, the author did careful develop a sample inputthat caused large
 portions of the code to execute (e.g. created high code coverage).  This guided
 development until the code eventually produced a “good” output given that input.
  The author then treated this (input, output) pair as an ad hoc functional test.
  As time went on and the project grew, the internal parts of the system because
 harder mentally track.  The author desired to refactor the code, but the
 increased complexity of the code base sometimes made it unclear exactly what
 each function was doing.  A unit test suite with clearly defined inputs and
 outputs would have helped refactor the code for improved readability,
 organization, and efficiency, not to mention improve comments/documentation.  As
 it was, the sole ad hoc test left the inner workings of the code an opaque black
 box to any other developer aside from the author.

Realizing that the functional test was already executing large portions of the
 code, apparently accurately, the author determined that unit tests could be
 created programmatically by assuming that all code executing under the
 functional test was in fact correct, then using the inputs and outputs from each
 function called during the functional test as the inputs and outputs for the
 unit tests.  The author acknowleges that software engineers may not 
 wish to assume that a apparently working functional test indicates accuracy of 
 all internally executed components. Regardless, the creation of such unit 
 tests would still provide a syntactically accurate unit
 test file for the software engineer to start from, rather than create each unit
 test from scratch.  In addition to saving time from creating the test
 boilerplate, the true expense saved is that of manually defining the desired
 inputs and correct outputs.  This author believes there is value in this
 approach even if the generated tests must still be verified for accuracy, as it
 should be faster to manually inspect code for accuracy than develop and verify
 such code from scratch.

Another assumption required for successful execution is that the repr() method
 of each object generates valid Python code than can be used to re-created that
 object, or that such a function can be developed and used to temporarily
 overwrite a repr() method not meeting this requirement.  The author demonstrates
 this in the repository code by overwriting the repr() method of the
 Pandas DataFrame class.